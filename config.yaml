# general
gpu_id: 0
use_gpu: True
seed: 19971003
state: INFO
dataset: webnlg
num_samples: 350
reproducibility: True
mode: train

# dataset
data_dir: "preprocess"
node_vocab: "preprocess/node.pkl"
relation_vocab: "preprocess/relation.pkl"
node_embedding: "preprocess/node_embeddings.npy"

# model
teacher_dir: "facebook/bart-base"
plm_dir: "facebook/bart-base"
log_dir: "logging"

# training settings
start_epoch: 0
epochs: 20
train_batch_size: 25
plm_learner: adamw
plm_lr: 0.00005
external_learner: adamw
external_lr: 0.00001
rec_weight: 1.0
kd_weight: 1.0
cp_weight: 0.5
gnn_layers: 1
embedding_size: 768
hidden_size: 768

# evaluation settings
eval_batch_size: 20

# testing settings
external_model: "./ckpt/webnlg-5-0/external.bin"
fine_tuned_plm_dir: "./ckpt/webnlg-5-0"
test_batch_size: 50
max_seq_length: 100
output_dir: "./ckpt/webnlg-5-0"
